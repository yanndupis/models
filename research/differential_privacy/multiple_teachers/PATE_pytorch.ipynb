{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning private models with multiple teachers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ressources:\n",
    "\n",
    "http://www.cleverhans.io/privacy/2018/04/29/privacy-and-machine-learning.html\n",
    "https://github.com/tensorflow/models/tree/master/research/differential_privacy/multiple_teachers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Protocol:\n",
    "1. Train teachers:\n",
    "    - Devide training set into buckets (not overlapping)\n",
    "    - Train a models (teacher) on each bucket\n",
    "2. Train student:\n",
    "    - Extract a share of the test set\n",
    "    - Ensemble predictions from teachers: queries each teacher for predictions on the test set share\n",
    "    - Aggregate teacher predictions to get student training labels using noising max: it\n",
    "  adds Laplacian noise to label counts and returns the most frequent label\n",
    "    - Train student with the aggregated label\n",
    "    - Validate the student model on the remaining test data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Teachers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py27/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"/Users/yanndupis/Documents/Datascience/private-ml/models/research/\"\n",
    "os.chdir(path)\n",
    "\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from differential_privacy.multiple_teachers import deep_cnn\n",
    "from differential_privacy.multiple_teachers import input\n",
    "from differential_privacy.multiple_teachers import metrics\n",
    "\n",
    "from torch.utils.data import DataLoader,Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "tf.flags.DEFINE_string('dataset', 'mnist', 'The name of the dataset to use')\n",
    "tf.flags.DEFINE_integer('nb_labels', 10, 'Number of output classes')\n",
    "\n",
    "tf.flags.DEFINE_string('data_dir','/tmp','Temporary storage')\n",
    "tf.flags.DEFINE_string('train_dir','/tmp/train_dir',\n",
    "                       'Where model ckpt are saved')\n",
    "\n",
    "tf.flags.DEFINE_integer('max_steps', 3000, 'Number of training steps to run.')\n",
    "tf.flags.DEFINE_integer('nb_teachers', 10, 'Teachers in the ensemble.')\n",
    "tf.flags.DEFINE_integer('teacher_id', 0, 'ID of teacher being trained.')\n",
    "\n",
    "\n",
    "FLAGS = tf.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data, test_labels = input.ld_mnist()\n",
    "\n",
    "# Reshape to have channel first\n",
    "train_data = train_data.reshape(60000,1,28,28)\n",
    "test_data = test_data.reshape(10000,1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n",
      "(60000,)\n",
      "(10000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, nb_teachers, teacher_id = FLAGS.dataset, FLAGS.nb_teachers,  FLAGS.teacher_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN_Model(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(CNN_Model, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 64, 5, stride=1)\n",
    "#         self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#         # Add local response normalization\n",
    "#         self.conv2 = nn.Conv2d(64, 128, 5, stride=1)\n",
    "#         # Add local response normalization\n",
    "#         self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "#         self.linear1 = nn.Linear(128*3*3,384)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "        \n",
    "#         self.linear2 = nn.Linear(384,192)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "        \n",
    "#         self.logit = nn.Linear(192, num_classes)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.maxpool1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.maxpool2(x)\n",
    "#         x = x.view(-1, 128*3*3)\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.relu1(x)\n",
    "#         x = self.linear2(x)\n",
    "#         x = self.relu2(x)\n",
    "#         out = self.logit(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5, stride = 1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.avgpool1 = nn.AvgPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 16, 5, stride = 1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(16)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.avgpool2 = nn.AvgPool2d(2)\n",
    "        self.linear1 = nn.Linear(256, 100)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(100)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(100, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.avgpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.avgpool2(x)\n",
    "        x = x.view(-1, 256)\n",
    "        x = self.linear1(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.relu3(x)\n",
    "        out = self.linear2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_Model(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, ckpt_path, filename):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    for epoch in range(10):\n",
    "            model.train() # set model to training mode\n",
    "\n",
    "            # set up training metrics we want to track\n",
    "            correct = 0\n",
    "            train_num = len(train_loader.sampler)\n",
    "\n",
    "            for ix, (img, label) in enumerate(train_loader): # iterate over training batches\n",
    "                #img, label = img.to(device), label.to(device) # get data, send to gpu if needed\n",
    "                img = img.type(torch.float32)\n",
    "                #label = label.type(torch.float32)\n",
    "                label = label.type(torch.LongTensor)\n",
    "                optimizer.zero_grad() # clear parameter gradients from previous training update\n",
    "                output = model(img) # forward pass\n",
    "                #output = output.type(torch.float32)\n",
    "                loss = F.cross_entropy(output, label,size_average=False) # calculate network loss\n",
    "                loss.backward() # backward pass\n",
    "                optimizer.step() # take an optimization step to update model's parameters\n",
    "\n",
    "                pred = output.max(1, keepdim=True)[1] # get the index of the max logit\n",
    "                correct += pred.eq(label.view_as(pred)).sum().item() # add to running total of hits\n",
    "\n",
    "            # print whole epoch's training accuracy; useful for monitoring overfitting\n",
    "            print('Train Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "                correct, train_num, 100. * correct / train_num))\n",
    "\n",
    "    if not os.path.isdir(ckpt_path):\n",
    "        os.makedirs(ckpt_path)\n",
    "\n",
    "    torch.save(model.state_dict(), ckpt_path + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_teachers(train_data, train_labels, nb_teachers, teacher_id):\n",
    "    \n",
    "    data, labels = input.partition_dataset(train_data,\n",
    "                                         train_labels,\n",
    "                                         nb_teachers,\n",
    "                                         teacher_id)\n",
    "    \n",
    "    train_prep = PrepareData(data, labels)\n",
    "    \n",
    "    train_loader = DataLoader(train_prep, batch_size=64, shuffle=True)\n",
    "    \n",
    "    print(\"\\nTrain teacher ID: \" + str(teacher_id))\n",
    "    train(model, train_loader, ckpt_path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train teacher ID: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py27/lib/python2.7/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 400/600 (67%)\n",
      "Train Accuracy: 551/600 (92%)\n",
      "Train Accuracy: 575/600 (96%)\n",
      "Train Accuracy: 588/600 (98%)\n",
      "Train Accuracy: 595/600 (99%)\n",
      "Train Accuracy: 595/600 (99%)\n",
      "Train Accuracy: 599/600 (100%)\n",
      "Train Accuracy: 597/600 (100%)\n",
      "Train Accuracy: 598/600 (100%)\n",
      "Train Accuracy: 599/600 (100%)\n",
      "\n",
      "Train teacher ID: 1\n",
      "Train Accuracy: 546/600 (91%)\n",
      "Train Accuracy: 575/600 (96%)\n",
      "Train Accuracy: 585/600 (98%)\n",
      "Train Accuracy: 598/600 (100%)\n",
      "Train Accuracy: 599/600 (100%)\n",
      "Train Accuracy: 599/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 599/600 (100%)\n",
      "\n",
      "Train teacher ID: 2\n",
      "Train Accuracy: 568/600 (95%)\n",
      "Train Accuracy: 585/600 (98%)\n",
      "Train Accuracy: 593/600 (99%)\n",
      "Train Accuracy: 596/600 (99%)\n",
      "Train Accuracy: 598/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 599/600 (100%)\n",
      "Train Accuracy: 598/600 (100%)\n",
      "Train Accuracy: 599/600 (100%)\n",
      "\n",
      "Train teacher ID: 3\n",
      "Train Accuracy: 572/600 (95%)\n",
      "Train Accuracy: 591/600 (98%)\n",
      "Train Accuracy: 596/600 (99%)\n",
      "Train Accuracy: 598/600 (100%)\n",
      "Train Accuracy: 599/600 (100%)\n",
      "Train Accuracy: 599/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "\n",
      "Train teacher ID: 4\n",
      "Train Accuracy: 573/600 (96%)\n",
      "Train Accuracy: 587/600 (98%)\n",
      "Train Accuracy: 594/600 (99%)\n",
      "Train Accuracy: 597/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 599/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 599/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "\n",
      "Train teacher ID: 5\n",
      "Train Accuracy: 580/600 (97%)\n",
      "Train Accuracy: 588/600 (98%)\n",
      "Train Accuracy: 596/600 (99%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 598/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 599/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 599/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "\n",
      "Train teacher ID: 6\n",
      "Train Accuracy: 571/600 (95%)\n",
      "Train Accuracy: 590/600 (98%)\n",
      "Train Accuracy: 598/600 (100%)\n",
      "Train Accuracy: 598/600 (100%)\n",
      "Train Accuracy: 598/600 (100%)\n",
      "Train Accuracy: 599/600 (100%)\n",
      "Train Accuracy: 597/600 (100%)\n",
      "Train Accuracy: 599/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "\n",
      "Train teacher ID: 7\n",
      "Train Accuracy: 573/600 (96%)\n",
      "Train Accuracy: 590/600 (98%)\n",
      "Train Accuracy: 597/600 (100%)\n",
      "Train Accuracy: 597/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "\n",
      "Train teacher ID: 8\n",
      "Train Accuracy: 579/600 (96%)\n",
      "Train Accuracy: 593/600 (99%)\n",
      "Train Accuracy: 595/600 (99%)\n",
      "Train Accuracy: 597/600 (100%)\n",
      "Train Accuracy: 599/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "\n",
      "Train teacher ID: 9\n",
      "Train Accuracy: 572/600 (95%)\n",
      "Train Accuracy: 591/600 (98%)\n",
      "Train Accuracy: 597/600 (100%)\n",
      "Train Accuracy: 597/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n",
      "Train Accuracy: 600/600 (100%)\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = 'differential_privacy/multiple_teachers/' + 'checkpoint/'\n",
    "\n",
    "for teacher_id in range(nb_teachers):\n",
    "    \n",
    "    filename = str(dataset) + '_' + str(nb_teachers) + '_teachers_' + str(teacher_id) + '.pth'\n",
    "\n",
    "    train_teachers(train_data, train_labels, 100, teacher_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_preds(images_loader, ckpt_path, return_logits=False):\n",
    "    \"\"\"\n",
    "    Compute softmax activations (probabilities) with the model saved in the path\n",
    "    specified as an argument\n",
    "    :param images: a np array of images\n",
    "    :param ckpt_path: a TF model checkpoint\n",
    "    :param logits: if set to True, return logits instead of probabilities\n",
    "    :return: probabilities (or logits if logits is set to True)\n",
    "    \"\"\"\n",
    "    # Compute nb samples and deduce nb of batches\n",
    "    data_length = len(images_loader.dataset)\n",
    "    preds = np.zeros((data_length, FLAGS.nb_labels), dtype=np.float32)\n",
    "    start = 0\n",
    "    \n",
    "    check = torch.load(ckpt_path)\n",
    "    model.load_state_dict(check)\n",
    "    model.eval() # set model to evaluate mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, label in images_loader:\n",
    "            output = model(img)\n",
    "            output_softmax = F.softmax(output).data.numpy()\n",
    "            \n",
    "            end = start + len(img)\n",
    "            \n",
    "            preds[start:end,:] = output_softmax\n",
    "            \n",
    "            start += len(img)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py27/lib/python2.7/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.96129870e-01, 1.64601408e-06, 2.19379726e-04, ...,\n",
       "        3.71756760e-05, 3.35881341e-04, 2.21464157e-04],\n",
       "       [9.99902964e-01, 2.03707623e-06, 8.10717320e-05, ...,\n",
       "        3.48550316e-06, 1.49741891e-06, 4.96857062e-08],\n",
       "       [2.34841827e-05, 4.04260441e-04, 2.81177017e-06, ...,\n",
       "        1.02101685e-05, 8.10734855e-05, 1.25520935e-04],\n",
       "       ...,\n",
       "       [2.08887202e-03, 1.78590519e-04, 6.77612145e-03, ...,\n",
       "        1.73116918e-03, 1.62030421e-02, 9.26935375e-02],\n",
       "       [6.17830853e-09, 9.99998450e-01, 3.21992445e-07, ...,\n",
       "        2.24134070e-07, 3.18366517e-07, 5.19417975e-09],\n",
       "       [1.53743476e-03, 7.17969425e-03, 2.01376592e-04, ...,\n",
       "        1.10462606e-05, 1.37406483e-03, 1.55625865e-04]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prep = PrepareData(test_data, test_labels)\n",
    "    \n",
    "test_loader = DataLoader(test_prep, batch_size=64, shuffle=True)\n",
    "\n",
    "softmax_preds(test_loader, './differential_privacy/multiple_teachers/checkpoint/mnist_10_teachers_0.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_preds(dataset, nb_teachers, stdnt_data_loader):\n",
    "  \"\"\"\n",
    "  Given a dataset, a number of teachers, and some input data, this helper\n",
    "  function queries each teacher for predictions on the data and returns\n",
    "  all predictions in a single array. (That can then be aggregated into\n",
    "  one single prediction per input using aggregation.py (cf. function\n",
    "  prepare_student_data() below)\n",
    "  :param dataset: string corresponding to mnist, cifar10, or svhn\n",
    "  :param nb_teachers: number of teachers (in the ensemble) to learn from\n",
    "  :param stdnt_data: unlabeled student training data\n",
    "  :return: 3d array (teacher id, sample id, probability per class)\n",
    "  \"\"\"\n",
    "\n",
    "  # Compute shape of array that will hold probabilities produced by each\n",
    "  # teacher, for each training point, and each output class\n",
    "  result_shape = (nb_teachers, len(stdnt_data_loader), FLAGS.nb_labels)\n",
    "\n",
    "  # Create array that will hold result\n",
    "  result = np.zeros(result_shape, dtype=np.float32)\n",
    "\n",
    "  # Get predictions from each teacher\n",
    "  for teacher_id in xrange(nb_teachers):\n",
    "    # Compute path of checkpoint file for teacher model with ID teacher_id\n",
    "    filename = str(dataset) + '_' + str(nb_teachers) + '_teachers_' + str(teacher_id) + '.pth'\n",
    "\n",
    "    # Get predictions on our training data and store in result array\n",
    "    result[teacher_id] = softmax_preds(stdnt_data_loader, ckpt_path)\n",
    "\n",
    "    # This can take a while when there are a lot of teachers so output status\n",
    "    print(\"Computed Teacher \" + str(teacher_id) + \" softmax predictions\")\n",
    "\n",
    "  return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
